# Objectives: 
In the context of nonconvex binary classification problems, this project aims to:
- investigate the numerical effects of the momentum parameter of stochastic methods;
- compare the different sampling schemes of stochastic methods.

# Algorithms Description:

- **Random Reshuffling with Momentum (RRM)** reshuffles the training data at each epoch and incorporates a momentum term.
The stochastic gradients generated by random reshuffling can be interpreted as *sampling without-replacement*, as each data point is used exactly once per epoch in a random order.

- **Incremental Gradient with Momentum (IGM)** processes the training data in a cyclic order and incorporates a momentum term. It can be seen as a deterministic version of RRM.

- **Shuffle Once with Momentum (SOM)** shuffles the training data once at the beginning and processes the data in the shuffled order for all subsequent epochs. It incorporates a momentum term in the update.

- **Stochastic Gradient Descent (SGD)** generates the stochastic gradient by *sampling with-replacement* from the training data.

- **Stochastic Gradient Descent with Momentum (SGDM)** is the momentum variant of SGD.

# Requirements
To run the code in this project, you need MATLAB (version R2018a or later). A demo script is provided to quickly test.

# License
This project is licensed under the MIT License.

# Contact
For questions or inquiries, please contact:

- GitHub: [Junwen-Qiu](https://github.com/Junwen-Qiu)
- Email: jwqiu.opt@gmail.com
